[{"authors":["admin"],"categories":null,"content":"I am currently a masters student at Cornell University doing research in computer science. My interests are in artificial intelligence, virtual/augmented reality, computational neuroscience, and evolutionary algorithms.\nI am also a top scholar achiever from Botswana, a pianist, an avid gamer (and game developer), and a dog person. Feel free to directly message or email  me about anything!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://amritamar.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently a masters student at Cornell University doing research in computer science. My interests are in artificial intelligence, virtual/augmented reality, computational neuroscience, and evolutionary algorithms.\nI am also a top scholar achiever from Botswana, a pianist, an avid gamer (and game developer), and a dog person. Feel free to directly message or email  me about anything!","tags":null,"title":"Amrit Amar","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f8324b86e30c02f1db0e5a1f31e88ef1","permalink":"https://amritamar.github.io/archive/slider/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/slider/","section":"archive","summary":"","tags":null,"title":"","type":"archive"},{"authors":null,"categories":null,"content":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9de239fbe1baa13cff72a1dbf2a30827","permalink":"https://amritamar.github.io/archive/hero/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/hero/","section":"archive","summary":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar","tags":null,"title":"Academic","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"77140deb814caf12eab3729ff0b56537","permalink":"https://amritamar.github.io/archive/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/skills/","section":"archive","summary":"","tags":null,"title":"Skills","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14e332d49095350db2bebd3a5300ab80","permalink":"https://amritamar.github.io/archive/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/experience/","section":"archive","summary":"","tags":null,"title":"Experience","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a15459c49ea526b336c9c2700601cea","permalink":"https://amritamar.github.io/archive/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/accomplishments/","section":"archive","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a035fa6ecd57a0d5ddf36e0649eb1954","permalink":"https://amritamar.github.io/archive/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/posts/","section":"archive","summary":"","tags":null,"title":"Recent Posts","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7655b370a1acda01108ca8bab3874ed8","permalink":"https://amritamar.github.io/archive/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/projects/","section":"archive","summary":"","tags":null,"title":"Projects","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6df377d73328fa8fdb8286e6a847ca8f","permalink":"https://amritamar.github.io/archive/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/people/","section":"archive","summary":"","tags":null,"title":"People","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01d61ff578b878f0d04711eee5922d8e","permalink":"https://amritamar.github.io/archive/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/talks/","section":"archive","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9f0af46d651200606dc51bec92138d5c","permalink":"https://amritamar.github.io/archive/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/featured/","section":"archive","summary":"","tags":null,"title":"Featured Publications","type":"archive"},{"authors":null,"categories":null,"content":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    This homepage section is an example of adding elements to the Blank widget.\nBackgrounds can be applied to any section. Here, the background option is set give a color gradient.\nTo remove this section, delete content/home/demo.md.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9c5d2036d30ea9de785a143cca16e39a","permalink":"https://amritamar.github.io/archive/demo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/demo/","section":"archive","summary":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    This homepage section is an example of adding elements to the Blank widget.","tags":null,"title":"Academic Kickstart","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bfcd325579a5cfd5b765b644b230b205","permalink":"https://amritamar.github.io/archive/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/tags/","section":"archive","summary":"","tags":null,"title":"Popular Topics","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"abb02d824d4fb5c0fb18ecdae2f6deae","permalink":"https://amritamar.github.io/archive/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/contact/","section":"archive","summary":"","tags":null,"title":"Contact","type":"archive"},{"authors":null,"categories":null,"content":"GeoScents is a game that pits online users against each other in a challenge to guess where a given location is in a certain amount of time. It is a remake of an old game called GeoSense, made by a reddit user, u/mattfel. I heard about this game through a r/webgames thread.\nDespite memorizing capitals once in my life, it turns out that I am very bad at the game. After losing a few rounds really badly, I decided to use an afternoon to make a program to beat the game for me. There were 3 things needed:\n Reading the place given to players Looking up where that place is (latitude/longitude coordinates) Moving the mouse to where the place is on the world map  I decided to use Python-tesseract, a wrapper for Google\u0026rsquo;s Tesseract OCR Engine. It can read image files and output the text on the images. To get the image itself, I used something called PyAutoGUI, a library that automates GUI features, to capture a screenshot given coordinates on the screen. To get those coordinates, I needed keyboard input and thus required pynput, a library that controls input devices. When setting up the program, the user has to give coordinates on the screen of where the text of the location to look up will show up. Thus, we can get an image of the text and passing it into pytesseract, we get the location in string.\nTo look up where the place is, I used a libary called geopy. Geopy has OpenStreetMap\u0026rsquo;s Nominatum which allows me to get the latitude and longitude of a place given an address. Thus, all it needed was a simple function to look up the latitude/longitude of a given location. The lookup took less than 1 second in most cases as well.\nFinally, to move the mouse to the given latitude and longitude on the GeoScents map, I used PyAutoGUI. However I ran into a problem - the map given wasn\u0026rsquo;t entirely correct (the equator was much lower than the center of the map). Because this was a hack and not something that needed immense precision (and because GeoScents lets you guess the location within a radius), I decided to forego precision for convenience and instead allowed the user to input 2 known locations and extrapolated where the mouse would be given the scaling. I chose the equator and the Cape of Good Hope as the two places needed to be selected. Once the screen coordinates are received, the program automatically finds the conversion from latitude/longitude coordinates to pixels and thus converts the location coordinates to pixel coordinates on screen.\nAll of the above is wrapped up into a button (in a GUID made with TkInter) which when clicked performs the above 3 steps and automatically moves your cursor to the location on the map. You can adjust for precision based on your knowledge but I just click because I trust the program more than my highschool geography. The program works well when calibrated correctly (correct coordinates for text, equator, and Cape of Good Hope) and achives really high scores! There are a couple of times where the program fails because of smaller text and propagated errors from the calibration. Currently the program only works for the world map. In the future, I hope I\u0026rsquo;ll be able to make it work for all the maps on GeoScents.\n","date":1578441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578441600,"objectID":"ab2fc9ee34732066cf1470ce235b29d6","permalink":"https://amritamar.github.io/project/geoscentshack/","publishdate":"2020-01-08T00:00:00Z","relpermalink":"/project/geoscentshack/","section":"project","summary":"A program I wrote to beat GeoScents.","tags":["Personal","Game Design"],"title":"Hacking GeoScents","type":"project"},{"authors":null,"categories":null,"content":"This is the final project my team and I worked on in graduate computer vision. I give a brief summary of what we did throughout the semester. Unfortunately, I can\u0026rsquo;t release the code as per Cornell regulations, but feel free to read the final paper on the project (linked above). All the following pictures were generated by computers!\nOur goal in this project is to see if it is possible to generate human poses using GANs. We draw inspiration from the paper, Generating Videos with Scene Dynamics, which proposed a Generative Adversarial Network (GAN) for videos that untangle the scene\u0026rsquo;s foreground from the background, also known as VideoGAN. VideoGAN generated videos that, while not photo-realistic, display motions that were fairly reasonable for the scenes it was trained on. Specifically, we explored generating poses by understanding human dancing, by training our models on the Let’s Dance Dataset. We evaluated how well VideoGAN is able to output dance video and we proposed a new architecture called VectorGAN that takes the vector representations of people dancing and generates videos.\nOur intention in this project was to explore how computers can capture motion and generate it. By generating motion from a given frame, we can generate \u0026lsquo;future\u0026rsquo; motion. There are lots of applications for generating “future” video such as video understanding and simulations and animation (specifically in Rigging avatars to automatically create ranges of motion). Given a starting image, we wanted to make predictions about certain events. We used dancing because human motion is fluid and defined in dancing, and the dataset we used was clearly categorized.\nWe implemented the following:\n VideoGAN Architecture - 32 Frames of Video Modified VideoGAN Architecture - 128 Frames of Video VectorGAN Architecture with Random Noise - 32 Frames of Video VectorGAN Architecture with Starting Vector - 32 Frames of Video VectorGAN Architecture Composited Video (using previous experiment) - 128 Frames of Video  VideoGAN: following the inspiration paper, Generating Videos with Scene Dynamics, we implemented their model and generated 32 frames of video. We then modified their architecture and generated 128 frames of video. Overall, from the output video, this model does well in making a figure of a human doing a dance move. However, it is very noisy. We expected this from a model that takes in raw pixels. This also happened in the second experiment where, even though the generated dance video was longer, it was still blurry. However, VideoGAN recognized people and captured some of their movements (blue is 32 frames, green is 128 frames).\nVectorGAN: using the dance video\u0026rsquo;s vector representation, we trained our model on the positions of human parts. Although VectorGAN had to be trained on more epochs to get something smooth, this method worked pretty well. The generated vector positions through time resemble a lot of the motion that ballet dancers do. It is interesting to note that the model figured out how the different limbs of humans connect to each other after about 100 epochs but needed much more to generate coherent movement. There are still a lot of places where the dancer’s limbs jerk, but we attribute this to the shortcomings of the dataset we used; not every single frame had a coherent human structure and some frames had nothing. We then tested the VectorGAN with an encoder added on to the beginning of the architecture that had the starting vectors of the input video. We then used this in the composited video experiment, where we generated 128 frame videos by feeding the last frame of a generated 32 frame video into the model 3 times (random noise, starting vector, and 128 frames in that order below).\nOverall, VideoGAN and VectorGAN provide tangible results in generating videos of human pose. However, the frames generated VideoGAN were of very low resolution as computing images of stick figures is still very difficult. VectorGAN had much better results as we could animate together distinct key points to see the figure move more clearly. Our conclusion is that generating human pose is possible with VectorGAN, but we don’t have nearly enough training data. The original VideoGAN paper used over 35 million clips while we used only 84 videos for training our models. However, the results we got are promising. We only used the ballet dances from the dataset, but it would be interesting (and very time consuming) to see how the model does on other dances.\n","date":1576713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576713600,"objectID":"55590df8f5ee2fb7d405c6f923e8e0dd","permalink":"https://amritamar.github.io/project/dancegans/","publishdate":"2019-12-19T00:00:00Z","relpermalink":"/project/dancegans/","section":"project","summary":"A GANs Project for CS 6670 (Graduate Computer Vision).","tags":["Cornell","AI"],"title":"Everybody GANs Now!","type":"project"},{"authors":null,"categories":null,"content":"This was a project I made after starting to learn javascript using p5.js. I really like the idea of how code can be used to visualize abstract math concepts (and show what exactly is happening in the background) and I hope to continue making more visualizations using code in the future. First a few references: I got interested in fourier transforms after watching 3Blue1Brown\u0026rsquo;s video on the topic. I also saw this really cool and interactive introduction to the topic by @jezzamonn. Finally, I learnt javascript (and most of this project) by watching Daniel Shiffman\u0026rsquo;s Coding Challenges!\nWhat is a fourier transform? It\u0026rsquo;s simply a way of breaking apart a wave into individual components. This is most used in signal processing, specifically in the audio area. One of the coolest things you can do with fourier transforms is extract specific sounds and tune them however you like! So for example, if you had a music clip with a bit of fuzzy noise in the background, you can use a fourier transform on the clip and get the individual sound waves. After finding the one causing the fuzzy noise, you can remove it. Combine the remaining waves and voila - you have a sound clip without that fuzzy sound.\nFollowing Daniel Shiffman\u0026rsquo;s tutorial, I first started with visualizing the fourier series. I visualized the square wave and the sawtooth wave. You can adjust the sliders to increase/reduce the number of epicycles, and use the dropdown menu to change the wave in question.\n\r Signals, or waves, can be interpreted as a set of points. One for the X axis, and one for the Y axis. If we apply the fourier transform to both of them, we can 2 transforms that give us the frequence, phase, and amplitude of each component. This is what I did in the next iteration!\n\r I then, instead of simply drawing a circle from a set of points, implemented the ability to create the signal/drawing yourself! Once you finish drawing, the program does a fourier transform, breaking your drawing into X and Y components. Try it out - click and draw something!\n\r The final part of this project involved combining the 2 parts, X and Y components, into 1 Complex Number component. This allows you to use only one overall circle to draw both parts of the transform as the complex number component captures both X and Y of the drawing. Try the final version out!\nIn addition:\n use A/D to reduce/increase the number of epicycles used\n use W/S to increase/reduce the speed\n  \u0026hellip; and see how it affects the fourier transform!\n\r I love the way code can be used to visualize math concepts. I hope to do more of these in the future, both to understand the math behind tricky concepts and to improve my javascript skills!\n","date":1565654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565654400,"objectID":"8c4ef68328e99f9a2aaba3b46a73997f","permalink":"https://amritamar.github.io/project/fouriertransforms/","publishdate":"2019-08-13T00:00:00Z","relpermalink":"/project/fouriertransforms/","section":"project","summary":"A fun mini-project I did to understand fourier transforms.","tags":["Personal"],"title":"Exploring and Understanding Fourier Transforms using p5.js","type":"project"},{"authors":null,"categories":null,"content":"This is an old project I recently ported over to JS. The original code for the project was written in Visual Basic 6, the first language I learnt. I returned to this project while learning JS (specifically using p5.js). It\u0026rsquo;s a 2 player game that adds an extra layer of thinking to the original game of tic tac toe.\n\r I hope to continue this by adding AI players!\n","date":1565568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565568000,"objectID":"c32c9cf73a8f11a87e6d290d6fccac82","permalink":"https://amritamar.github.io/project/ultimatetictactoe/","publishdate":"2019-08-12T00:00:00Z","relpermalink":"/project/ultimatetictactoe/","section":"project","summary":"A small game I made to learn more JS.","tags":["Personal","Game Design"],"title":"Ultimate Tic Tac Toe!","type":"project"},{"authors":null,"categories":null,"content":"A star has fallen from the sky and the race is on to go claim it. However, the monsters around the area will not make it easy. Players can choose to play as a warrior, rogue, or wizard to blast through enemies by choosing powerful spells or attacks and casting them as quickly as possible while dodging enemy blows to stay alive.\nStelliform is a real-time action card game that relies on quick swiping mechanics, split-second decision making, and pre-game planning. The integration of a real-time strategy card game with a stimulating swipe mechanic makes Stelliform a distinct and unique mobile game. By combining strategic card-based elements and real-time action, we want players to step into the shoes of a spellcaster, defeating enemies by constructing a spell deck and planning and casting spells under time pressure.\nThe game was produced by Team Sicko Code, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Luke Shin (Programmer) Brandon Zhang (Programmer) Hanna Arfine (UI/UX Artist) Ally Yuan (Character Artist) Michael Yee (Music)  The game is made using CUGL (Cornell University Graphics Library), a game engine designed by Professor Walker White in C++. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in C++, along with Android Building and testing.\n\r\r\r\r\r\r\r ","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558051200,"objectID":"db90a4a922d33ad1d008a9bb887be83f","permalink":"https://amritamar.github.io/project/stelliform/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/project/stelliform/","section":"project","summary":"An action, strategy mobile game my team made for CS 4152 (Advanced Game Design).","tags":["Game Design","Android","Cornell"],"title":"Stelliform","type":"project"},{"authors":null,"categories":null,"content":"You\u0026rsquo;ve crashed on a mysterious world after getting caught in an explosion. You\u0026rsquo;ve lost the shards that allowed you to fly and explore the galaxy. Make your way through different environments to find the shards and escape from the planet!\nViridian is an action webgame made that relies on precise mechanics to make your way through a magical forest. Use your mouse to shoot and gain momentun as you travel through 3 biomes. Beware of shooting plants, however, as the forest is connected to each other and plants will actively try to defend their fellow flora.\nThe game was produced by Team LVI Studios, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Justin Lue (Programmer) Brandon Zhang (Programmer) Yanying (Mary) Ji (UI/UX Artist) Urael Xu (Character Artist)  The game is made using Unity Game Engine. Throughout development, the team released several builds of the game and gathered data using A/B testing on statistics such as player deaths, time spent on levels, and other indicators that the team used to incrementally build a better game. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented the levels and graphics in C#.\n\r\r\r\r\r ","date":1544918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544918400,"objectID":"5f68375a27770bf6689394ca77bb43a1","permalink":"https://amritamar.github.io/project/viridian/","publishdate":"2018-12-16T00:00:00Z","relpermalink":"/project/viridian/","section":"project","summary":"An action web game my team made for CS 4154 (Analytical Game Design).","tags":["Game Design","Web","Cornell"],"title":"Viridian","type":"project"},{"authors":null,"categories":null,"content":"This is a project I did to learn about Neural Networks and Genetic Algorithms. We create a neural network to drive a car and then use genetic algorithms to train and find the best weights for the neural network. The network takes in 6 inputs and outputs the acceleration and steering of the car.\nHere, we see all the cars beginning to get initialized with a random neural network. They are pretty dumb.\nIn the editor, this is the information that cars get (as shown by the yellow lines).\nBy generation 22, we have a car that can finish the whole track (albeit not smoothly)!\nOn average, it takes about 22 generations to train the neural network fully.\nThe neural network has 6 input nodes:\n current speed distance of closest obstacle from the left side (max 10m) distance of closest obstacle from the right side (max 10m) distance of closest obstacle from the front (max 10m) distance of closest obstacle from the front-left side (max 10m) distance of closest obstacle from the front-right side (max 10m)  There is 1 hidden layer, consisting of 5 nodes. There is 1 output layer, consisting of 2 nodes:\n Acceleration (-1 to 1) Steering (-1 to 1)  I use tanH as my activation function.\nThere are 3 main scripts:\n CarController: this is the script for a car. Has a neural network that drives the car. CarManager: manager for the population of cars NeuralNetwork: holds the neural network code GeneticAlgorithmController: performs the selection process of the algorithm The program works as follows:  The input and hidden layer nodes have both a weight and a bias, that is randomly generated for the population of cars (default 10). They are then simulated as generation 1 on the race track. After all the cars crash or 60 seconds, the algorithm chooses the best 2 cars (based on displacement from starting point) and then creates the next generation of cars by combining the best 2 cars genes (randomly selecting between the 2 or a combination thereof). There is an option that allows us to keep the best 2 cars in the next generation or to generate all new cars in the population from the parents (i.e. the parents die). The next generation is then simulated and so on.\n","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"ce49541bf0ac474129da0ae7da631934","permalink":"https://amritamar.github.io/project/geneticselfdrivingcars/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/project/geneticselfdrivingcars/","section":"project","summary":"A project I made to explore neuroevolution.","tags":["Personal","AI"],"title":"Genetic Self-Driving Cars","type":"project"},{"authors":null,"categories":null,"content":"This is a JS implementation of a particle system. The project simulates particles in a \u0026lsquo;box\u0026rsquo; and assigns random forces between different types of particles. Interestingly, certain behaviors arise that are \u0026lsquo;life-like\u0026rsquo; from these rules. The particles interact with each other arising in behaviors that mimic cell behavior.\nThis simulation works by having each particle type have a certain force associated with another particle type. Sometimes the force can attract, sometimes it repels. The force is stronger the closer the two particles are, but after a certain point, they do not affect each other. The randomization of these forces leads to a number of interesting behaviors. You can play with the simulation in the widget below!\nYou can control the number of particles (and click to add more of a certain type), the interaction radius controls the distance at which particles interact with each other (a global variable here), and a few other options. Feel free to edit the code and try out other values in the web code editor.\n\r I was inspired by this from an Evolution Class I took at Cornell where I read the theories of biologist Lynn Margulis. I was further inspired to create my own implementation from Jeffrey Ventrella\u0026rsquo;s Clusters, which has examples in both the browser and in augmented reality. Furthermore, I also drew inspiration from this study and HackerPoet and his Particle-Life Video. The main idea, from Lynn Margulis is that all organisms naturally gravitate to other kinds of organisms, and likewise are repelled by different organisms. These particles experience attractions and repulsions with other particles of different colors. They cluster into social pods, or scatter and flee, often mimicking biological behaviors. Clusters shapeshift and exchange parts of their identities, much like the tiny organisms that came together in early evolution to form symbiotic unions.\n","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"973ca48bf103ed0e2771ae8d8309a6b2","permalink":"https://amritamar.github.io/project/particlesimulationoflife/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/project/particlesimulationoflife/","section":"project","summary":"A project I made to explore the theories by biologist Lynn Margulis.","tags":["Personal","AI"],"title":"A Particle Simulation of Life","type":"project"},{"authors":null,"categories":null,"content":"You are a lab assistant who has accidentally shattered the fabric of reality and created numerous rifts between worlds. Time, gravity, and even color have been drained from the world. Now you must travel through these rifts, recover the scattered shards of space, and restore them to the riftss in order to fix the damage to reality.\nOutOfSync is a action, strategy platformer that relies on quick decision making and mechanics. You must plan your route and be quick as you collect the shards, return back to repair the rifts, and move to the next level. However, as you are doing this, clones of you will take your exact path - collide with them and you lose the game.\nThe game was produced by Team Tektite, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Luke Shin (Programmer) Brandon Zhang (Programmer) Cathy Liu (UI/UX Artist) Sarah Skrutskie (Character Artist)  Special thanks to Adam Pascual for an amazing soundtrack!\nThe game is made using LibGDX and Box2D. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in Java, along with building and testing the game.\n\r\r\r\r\r ","date":1526428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526428800,"objectID":"a8b90b2077b67ec17cabfd19cb449a66","permalink":"https://amritamar.github.io/project/outofsync/","publishdate":"2018-05-16T00:00:00Z","relpermalink":"/project/outofsync/","section":"project","summary":"A strategic, action platformer my team made for CS 3152 (Beginners Game Design).","tags":["Game Design","PC","Cornell"],"title":"OutOfSync","type":"project"},{"authors":null,"categories":null,"content":" This is a project I made for my Artificial Intelligence Class under the Team \u0026ldquo;Real Intelligence\u0026rdquo;. It is a image generation program that uses primitive shapes to generate images using genetic algorithms.\nOur intention was to complete a project exploring the concept of genetic algorithms within artificial intelligence. Genetic Algorithms use the concept of natural selection to find a minima to a function. In this case, the genetic algorithm tries to replicate a picture using only primitive shapes and tries to bring the generated picture as close to the real picture as possible.\nTo generate the image, we can adjust the number of shapes, the mutation rate, and the types/combinations of shapes to use in the generated image: - Circles (mutate the position, radius, color) - Ellipses (mutate the position, width, height, color) - Rectangles (mutate the position, width, height, color) - Pixels (n-pixels long squares, mutate the pixel position, color) - n-sided polygons\nOur implementation focuses on using a single-parent. This means that during each generation, only one picture is used to create all the children. A parent is replaced when one of the children has a better fitness score (or worse, rather). Our fitness function work by comparing the child to the original image pixel by pixel.\nThe pseudocode for our genetic algorithm is as follows:\n Generate a random image based on user-defined configurations, mainly number of shapes and types of shapes to use. Every ‘individual’ image has a ‘DNA’ that is a collection of shapes. These shapes are randomly generated and are assigned random colors. This individual image’s fitness score is also calculated here. We then generate a new image with a certain mutation rate. This new image will have random differences from the parent image (differences ranging from changing the shapes’ position or dimensions to changing the shapes’ color). The new image is then compared with the original image. We then compare the new image’s fitness score with the parent’s fitness score. If the image’s fitness score is lower than that of the parent image (i.e, there is less error in this image, thus the new image is closer to the original picture than the parent image), then we update the parent image to be the new image. If the fitness score is higher than the parent’s score, then we simply keep the same parent for the next generation. Go to part 2 and repeat until fitness score of the parent is 0.  Example 1: Eiffel Tower: Target Picture: Starting the Algorithm: After a bit of time: After a long time: Notice how the sky gradient is captured by the generated picture, simply by the adjustment of the alpha of overlapping shapes!\nOver time, the error in the generated images decreases. Of course, this happens over thousands of generations. As we used our program with a multitude of images, we noticed that some shapes converge to target images better than other shapes. For the full evaluation, check out our report.\nExample 2: Mona Lisa: Target Picture: Starting the Algorithm: After a bit of time: After a long time: Surprisingly, the algorithm seems to capture the background really well! Of course, facial features will take quite a while to develop as there is not much of a pixel difference from this image and one with facial features.\n","date":1525910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525910400,"objectID":"e0a1d0dbe530c5e5b230b8501a15d129","permalink":"https://amritamar.github.io/project/geneticimages/","publishdate":"2018-05-10T00:00:00Z","relpermalink":"/project/geneticimages/","section":"project","summary":"A project I made for CS 4701 (Practicium in Artificial Intelligence).","tags":["Cornell","AI"],"title":"Genetic Images","type":"project"},{"authors":null,"categories":null,"content":"This is a hand that can be used to do a lot of things. The basic motive of the project is that the user will wear the control glove, and will be able to control the robotic hand using their movements. A close friend and I made a robotic animatronic hand that could potentially help save lives in the event of an accident in Mines, one of Botswana\u0026rsquo;s main industries.\nThere are many uses for this such as Space Exploration and in the Mining Industry. Instead of using AI, that can\u0026rsquo;t react to all situations in an environment, we can use robots, that are controlled by the user. Humans can react to sudden environmental changes and, using \u0026ldquo;The Hand\u0026rdquo;, they can improvise and adapt.\nFor making the Project, we used an Arduino Leonardo, Flex Sensors and Servo Motors. The Arduino is a microprocessor that basically reads from code uploaded to it from a computer (C/C++ Code) and therefore through the use of Digital and Analog I/O pins, executes the code. Flex Sensors are Variable Resistors that change values depending on how much they are bent by. Servo Motors are precision DC motors that can be controlled to point to the exact angle from 0 to 180 Degrees.\nWhat basically happens here is that the Flex Sensor readings are inputs through the Analog Pins. It is processed and mapped to a angle. The angle is then written to the Servo Motors, thus turning them and therefore moving the fingers. There are also two buttons that control a LED and a Speaker. On the breadboard, there is an LDR (Light dependent resistor) that basically measure the amount of light in a room. Using those values, it gives a green light or a yellow light. The Arduino can be used to do a lot of things!\nIn the future, we hope to develop a better working hand with all fingers working and an opposable thumb. This would allow us to hold things. The next stage of this project is to finish the hand and make sure it can pick up things.\nWe won first place at a National Competition (The Botho College ICT Linkz Challenge 2014) with this project. Thank you to everyone who helped out!\nA Video Demonstration can be found here\n","date":1411257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1411257600,"objectID":"9c6554c5387d14ed6c13af7ba653318c","permalink":"https://amritamar.github.io/project/the-hand/","publishdate":"2014-09-21T00:00:00Z","relpermalink":"/project/the-hand/","section":"project","summary":"An IT Innovation project I made for the Botho College ICT Challenge, a national competition.","tags":["Arduino","Personal"],"title":"The Hand","type":"project"},{"authors":null,"categories":null,"content":"Youcandoit!\r\r\rI believe in you! If you can\u0026rsquo;t believe in yourself, believe in the me who believes in you! You\u0026rsquo;re handsome and you\u0026rsquo;re smart and you\u0026rsquo;re the best.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01da1065de75311039b9e5a832d8ffab","permalink":"https://amritamar.github.io/moralsupport/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/moralsupport/","section":"","summary":"moralsupport","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://amritamar.github.io/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://amritamar.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Experiences","tags":null,"title":"AmritAmar","type":"widget_page"}]