<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AMRIT AMAR</title>
    <link>https://amritamar.github.io/</link>
    <description>Recent content on AMRIT AMAR</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 May 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://amritamar.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://amritamar.github.io/archive/slider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/slider/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Academic</title>
      <link>https://amritamar.github.io/archive/hero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/hero/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Build &lt;strong&gt;Anything&lt;/strong&gt; with Widgets&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;text-shadow: none;&#34;&gt;&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/gcushen/hugo-academic&#34; data-icon=&#34;octicon-star&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Star this on GitHub&#34;&gt;Star&lt;/a&gt;&lt;script async defer src=&#34;https://buttons.github.io/buttons.js&#34;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skills</title>
      <link>https://amritamar.github.io/archive/skills/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>https://amritamar.github.io/archive/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accomplish&amp;shy;ments</title>
      <link>https://amritamar.github.io/archive/accomplishments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/accomplishments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Posts</title>
      <link>https://amritamar.github.io/archive/posts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://amritamar.github.io/archive/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://amritamar.github.io/archive/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://amritamar.github.io/archive/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Featured Publications</title>
      <link>https://amritamar.github.io/archive/featured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/featured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Academic Kickstart</title>
      <link>https://amritamar.github.io/archive/demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/demo/</guid>
      <description>&lt;p&gt;Welcome to the &lt;strong&gt;Academic Kickstart&lt;/strong&gt; template!&lt;/p&gt;

&lt;p&gt;Follow our &lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;Getting Started&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/docs/widgets/&#34; target=&#34;_blank&#34;&gt;Page Builder&lt;/a&gt; guides to easily personalize the template and then &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/&#34; target=&#34;_blank&#34;&gt;add your own content&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For inspiration, check out &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#demo-content&#34; target=&#34;_blank&#34;&gt;the Markdown files&lt;/a&gt; which power the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;personal demo&lt;/a&gt;. The easiest way to publish your new site to the internet is with &lt;a href=&#34;https://sourcethemes.com/academic/docs/deployment/&#34; target=&#34;_blank&#34;&gt;Netlify&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;View the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://discuss.gohugo.io/&#34; target=&#34;_blank&#34;&gt;Ask a question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34;&gt;Request a feature or report a bug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Updating? View the &lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support development of Academic:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34;&gt;Donate a coffee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34;&gt;Become a backer on Patreon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34;&gt;Decorate your laptop or journal with an Academic sticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34;&gt;Wear the T-shirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;This homepage section is an example of adding &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;elements&lt;/a&gt; to the &lt;a href=&#34;https://sourcethemes.com/academic/docs/widgets/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Blank&lt;/em&gt; widget&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Backgrounds can be applied to any section. Here, the &lt;em&gt;background&lt;/em&gt; option is set give a &lt;em&gt;color gradient&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To remove this section, delete &lt;code&gt;content/home/demo.md&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Popular Topics</title>
      <link>https://amritamar.github.io/archive/tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/tags/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://amritamar.github.io/archive/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/archive/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visualizing Language over Noise</title>
      <link>https://amritamar.github.io/project/visualizinglanguageovernoise/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/visualizinglanguageovernoise/</guid>
      <description>&lt;p&gt;This is the final project my team and I worked on in advanced artificial intelligence. I give a brief summary of what we did throughout the semester. Unfortunately, I can&amp;rsquo;t release the code as per Cornell regulations, but feel free to read the final paper on the project (linked above), along with the data. All the following pictures were generated by computers!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;8x8.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Communication isn’t perfect. Perhaps the room is too noisy or one party is distracted with another task but people often don’t understand each other perfectly; however, communication between people doesn’t break down. Similarly, in writing, we can still make out what the author intended to write on a piece of paper despite stains or wear down.  We explore the question, &lt;em&gt;can neural networks encode a language robust enough to be transmitted through noise channels and recreate the original meaning?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our goal in this project was to study the robustness of neural networks in encoding a language to communicate with one another. We used neural networks to generate images given vectors and transmitted those images over a noisy channel of communication.
Another neural network received these images and decoded the image back to its vector representation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;CommunicationModel.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our model is based on the Shannon-Weaver model of communication where the transmitter and receiver roles are played by two neural networks. G encodes a message. D decodes that message. As the networks get better at communicating, we increase the noise in the communication channel.&lt;/p&gt;

&lt;p&gt;We did the following experiments:
- Compare Grayscale and RGB output of networks
- Compare the impact of the size of output images between Grayscale models
- Compare the impact of the size of output images between RGB models
- The impact of noise on a larger dataset on grayscale networks.
- Qualitative Evaluation of RGB 64x64 Model with GloVe vectors&lt;/p&gt;

&lt;p&gt;The first picture in this article belongs to a greyscale model outputting 36 8x8 images (A-Z, 0-9). The picture below is a color model outputting 36 8x8 images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;8x8Color.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Example 64x64 pictures. Greyscale:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;64x64.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Color:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;64x64Color.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We also did word embeddings using Twitter&amp;rsquo;s GloVe embeddings:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;WordEmbeddingColor.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An explanation of our experiments and more data can be found in the source can be found on our source page above.&lt;/p&gt;

&lt;p&gt;Overall, qualitatively, our models created images that look distinct from one another. For several of the one-hot models, we can clearly see differences between various encodings, similar to how we see differences between characters of various alphabets. The difference between real life communication and our model is that real life communication is based on alphabets that are defined by outlines and real images, while our models simply try to fill in a box with colors. It would be interesting to extend this project to generate scribbles rather than defined images. Interestingly, the lower the dimension of the generated image, the closer it resembles our own letters. Perhaps with smaller dimensions and more limitations, we could get something that resembles our own alphabet.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Games2Anime</title>
      <link>https://amritamar.github.io/project/games2anime/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/games2anime/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://games2anime.herokuapp.com/&#34; target=&#34;_blank&#34;&gt;Games2Anime is a website that recommends Anime based on Steam Games.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Due to current world events (COVID-19), there is a greater demand for online entertainment. Steam recently reached a record high for concurrent users and more people than ever are consuming online entertainment. However, there are only so many games on Steam and many gamers may seek to broaden their horizons by trying something new, like Japanese animation (anime). However, it can be difficult to narrow what anime to watch due to its wide variety of genres and styles.&lt;/p&gt;

&lt;p&gt;Thus, the goal of our application is to recommend anime to people based on their Steam gaming preferences. By giving our system a game title from Steam, it will return a list of recommended anime based on the similarity in reviews and game description. Each recommendation displays general details and a similarity rating, along with a link to its &lt;a href=&#34;https://myanimelist.net/&#34; target=&#34;_blank&#34;&gt;MyAnimeList&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;It works by comparing Steam Game Descriptions and Anime User Reviews to give you a list of Anime recommendations.&lt;/p&gt;

&lt;p&gt;Games2Anime gets a list of all Anime with a MAL rating of 7.0 or above. We then remove any sequels/related movies from the list (to ensure we don&amp;rsquo;t recommend more than 1 show from a particular series on a search). We get the top 20 reviews for each anime in our list (sorted by most helpful reviews first). We then combine all the reviews and use a fancy tool called &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html&#34; target=&#34;_blank&#34;&gt;TfidfVectorizer&lt;/a&gt; that converts a collection of raw documents to a matrix of TF-IDF features. We then perform dimensionality reduction using singular value decomposition (&lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html&#34; target=&#34;_blank&#34;&gt;svds&lt;/a&gt;) to get a matrices of words in reviews and anime based on words. We these structures, we can find similar anime to words using cosine similarity.&lt;/p&gt;

&lt;p&gt;We then get a list of all Steam Games and their descriptions. Based on the game input, we go through each word in the game&amp;rsquo;s description and get the top anime to that word and build up a ranking based on how often an anime shows up across the entire description. After normalizing and sorting the scores, we return the top 5 anime recommendations based on the game.&lt;/p&gt;

&lt;p&gt;Because we are heavily reliant on keyword matching, some keywords get over weighted and thus we have to manually decrease the weight of certain keywords. This is ongoing so some search results are not great, yet.&lt;/p&gt;

&lt;p&gt;This was a fun final project for CS 4300! I learnt a lot about information retrieval methods and systems along the way and found my next show. I hope it finds your next show as well!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Circle Packing using p5.js</title>
      <link>https://amritamar.github.io/project/circlepacking/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/circlepacking/</guid>
      <description>&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/UVXX94iTP&#34; width=&#34;704&#34; height=&#34;704&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;This was a tiny project I made cause circles are cool! Wikipedia has a great explanation on how to &lt;a href=&#34;https://en.wikipedia.org/wiki/Circle_packing&#34; target=&#34;_blank&#34;&gt;pack circles&lt;/a&gt;. The source for circle packing is &lt;a href=&#34;https://editor.p5js.org/AmritAmar/sketches/UVXX94iTP&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and for the image version, &lt;a href=&#34;https://editor.p5js.org/AmritAmar/sketches/oL0gKBOyS&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;! Feel free to try out different parameters and images in the browser (just change imgSource to point to another picture). Warning - can lag on really big images with different parameters!&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/oL0gKBOyS&#34; width=&#34;704&#34; height=&#34;750&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The 3 sliders are for &lt;em&gt;counts per update&lt;/em&gt; or how many circles are added every frame (1 - 100), &lt;em&gt;total circles&lt;/em&gt; or max amount of circles on the screen (0 (= infinity) - 5000), and &lt;em&gt;total attempts&lt;/em&gt; or the number of times we try to add a circle before we cut off the algorithm (100 - 2000), respectively. By default, the values are 10, 2500, 1000, but you can vary them!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Visual BF Interpreter</title>
      <link>https://amritamar.github.io/project/bfvisualinterpreter/</link>
      <pubDate>Sat, 25 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/bfvisualinterpreter/</guid>
      <description>&lt;p&gt;This is a JS implementation of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Brainfuck&#34; target=&#34;_blank&#34;&gt;BrainFuck&lt;/a&gt; interpreter. I used P5JS to create this to understand how interpreters work and, more specifically, how this monstrosity of a programming language works. &lt;a href=&#34;https://editor.p5js.org/AmritAmar/sketches/T7UagDjHq&#34; target=&#34;_blank&#34;&gt;Mess with the code here&lt;/a&gt;!&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/T7UagDjHq&#34; width=&#34;724&#34; height=&#34;754&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hacking GeoScents</title>
      <link>https://amritamar.github.io/project/geoscentshack/</link>
      <pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/geoscentshack/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://geoscents.net/&#34; target=&#34;_blank&#34;&gt;GeoScents&lt;/a&gt; is a game that pits online users against each other in a challenge to guess where a given location is in a certain amount of time. It is a remake of an old game called GeoSense, made by a reddit user, &lt;a href=&#34;https://www.reddit.com/user/mattfel&#34; target=&#34;_blank&#34;&gt;u/mattfel&lt;/a&gt;. I heard about this game through a &lt;a href=&#34;https://www.reddit.com/r/WebGames/comments/ehs0ie/geoscents_an_online_world_geography_game/&#34; target=&#34;_blank&#34;&gt;r/webgames thread&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Despite memorizing capitals once in my life, it turns out that I am very bad at the game. After losing a few rounds really badly, I decided to use an afternoon to make a program to beat the game for me. There were 3 things needed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reading the place given to players&lt;/li&gt;
&lt;li&gt;Looking up where that place is (latitude/longitude coordinates)&lt;/li&gt;
&lt;li&gt;Moving the mouse to where the place is on the world map&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeoScentsHack/master/GeoScents3.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I decided to use &lt;a href=&#34;https://pypi.org/project/pytesseract/&#34; target=&#34;_blank&#34;&gt;Python-tesseract&lt;/a&gt;, a wrapper for Google&amp;rsquo;s Tesseract OCR Engine. It can read image files and output the text on the images. To get the image itself, I used something called &lt;a href=&#34;https://pypi.org/project/PyAutoGUI/&#34; target=&#34;_blank&#34;&gt;PyAutoGUI&lt;/a&gt;, a library that automates GUI features, to capture a screenshot given coordinates on the screen. To get those coordinates, I needed keyboard input and thus required &lt;a href=&#34;https://pypi.org/project/pynput/&#34; target=&#34;_blank&#34;&gt;pynput&lt;/a&gt;, a library that controls input devices. When setting up the program, the user has to give coordinates on the screen of where the text of the location to look up will show up. Thus, we can get an image of the text and passing it into pytesseract, we get the location in string.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeoScentsHack/master/GeoScents2.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To look up where the place is, I used a libary called &lt;a href=&#34;https://pypi.org/project/geopy/&#34; target=&#34;_blank&#34;&gt;geopy&lt;/a&gt;. Geopy has &lt;a href=&#34;https://wiki.openstreetmap.org/wiki/Nominatim&#34; target=&#34;_blank&#34;&gt;OpenStreetMap&amp;rsquo;s Nominatum&lt;/a&gt; which allows me to get the latitude and longitude of a place given an address. Thus, all it needed was a simple function to look up the latitude/longitude of a given location. The lookup took less than 1 second in most cases as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeoScentsHack/master/GeoScents4.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, to move the mouse to the given latitude and longitude on the GeoScents map, I used PyAutoGUI. However I ran into a problem - the map given wasn&amp;rsquo;t entirely correct (the equator was much lower than the center of the map). Because this was a hack and not something that needed immense precision (and because GeoScents lets you guess the location within a radius), I decided to forego precision for convenience and instead allowed the user to input 2 known locations and extrapolated where the mouse would be given the scaling. I chose the equator and the Cape of Good Hope as the two places needed to be selected. Once the screen coordinates are received, the program automatically finds the conversion from latitude/longitude coordinates to pixels and thus converts the location coordinates to pixel coordinates on screen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;geoscentsGUI.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All of the above is wrapped up into a button (in a GUID made with TkInter) which when clicked performs the above 3 steps and automatically moves your cursor to the location on the map. You can adjust for precision based on your knowledge but I just click because I trust the program more than my highschool geography. The program works well when calibrated correctly (correct coordinates for text, equator, and Cape of Good Hope) and achives really high scores! There are a couple of times where the program fails because of smaller text and propagated errors from the calibration. Currently the program only works for the world map. In the future, I hope I&amp;rsquo;ll be able to make it work for all the maps on GeoScents.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeoScentsHack/master/GeoScents5.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everybody GANs Now!</title>
      <link>https://amritamar.github.io/project/dancegans/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/dancegans/</guid>
      <description>&lt;p&gt;This is the final project my team and I worked on in graduate computer vision. I give a brief summary of what we did throughout the semester. Unfortunately, I can&amp;rsquo;t release the code as per Cornell regulations, but feel free to read the final paper on the project (linked above). All the following pictures were generated by computers!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;BalletStance.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our goal in this project is to see if it is possible to generate human poses using GANs. We draw inspiration from the paper, &lt;a href=&#34;http://www.cs.columbia.edu/~vondrick/tinyvideo/&#34; target=&#34;_blank&#34;&gt;Generating Videos with Scene Dynamics&lt;/a&gt;, which proposed a Generative Adversarial Network (GAN) for videos that untangle the scene&amp;rsquo;s foreground from the background, also known as VideoGAN. VideoGAN generated videos that, while not photo-realistic, display motions that were fairly reasonable for the scenes it was trained on. Specifically, we explored generating poses by understanding human dancing, by training our models on the &lt;a href=&#34;https://www.cc.gatech.edu/cpl/projects/dance/&#34; target=&#34;_blank&#34;&gt;Let’s Dance Dataset&lt;/a&gt;. We evaluated how well VideoGAN is able to output dance video and we proposed a new architecture called VectorGAN that takes the vector representations of people dancing and generates videos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;BalletMove1.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our intention in this project was to explore how computers can capture motion and generate it. By generating motion from a given frame, we can generate &amp;lsquo;future&amp;rsquo; motion. There are lots of applications for generating “future” video such as video understanding and simulations and animation (specifically in Rigging avatars to automatically create ranges of motion). Given a starting image, we wanted to make predictions about certain events. We used dancing because human motion is fluid and defined in dancing, and the dataset we used was clearly categorized.&lt;/p&gt;

&lt;p&gt;We implemented the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VideoGAN Architecture - 32 Frames of Video&lt;/li&gt;
&lt;li&gt;Modified VideoGAN Architecture - 128 Frames of Video&lt;/li&gt;
&lt;li&gt;VectorGAN Architecture with Random Noise - 32 Frames of Video&lt;/li&gt;
&lt;li&gt;VectorGAN Architecture with Starting Vector - 32 Frames of Video&lt;/li&gt;
&lt;li&gt;VectorGAN Architecture Composited Video (using previous experiment) - 128 Frames of Video&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VideoGAN: following the inspiration paper, &lt;a href=&#34;http://www.cs.columbia.edu/~vondrick/tinyvideo/&#34; target=&#34;_blank&#34;&gt;Generating Videos with Scene Dynamics&lt;/a&gt;, we implemented their model and generated 32 frames of video. We then modified their architecture and generated 128 frames of video. Overall, from the output video, this model does well in making a figure of a human doing a dance move. However, it is very noisy. We expected this from a model that takes in raw pixels. This also happened in the second experiment where, even though the generated dance video was longer, it was still blurry. However, VideoGAN recognized people and captured some of their movements (blue is 32 frames, green is 128 frames).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;videogan32.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;videogan128.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VectorGAN: using the dance video&amp;rsquo;s vector representation, we trained our model on the positions of human parts. Although VectorGAN had to be trained on more epochs to get something smooth, this method worked pretty well. The generated vector positions through time resemble a lot of the motion that ballet dancers do. It is interesting to note that the model figured out how the different limbs of humans connect to each other after about 100 epochs but needed much more to generate coherent movement. There are still a lot of places where the dancer’s limbs jerk, but we attribute this to the shortcomings of the dataset we used; not every single frame had a coherent human structure and some frames had nothing. We then tested the VectorGAN with an encoder added on to the beginning of the architecture that had the starting vectors of the input video. We then used this in the composited video experiment, where we generated 128 frame videos by feeding the last frame of a generated 32 frame video into the model 3 times (random noise, starting vector, and 128 frames in that order below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vecgan32.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vecgan232.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vecgan128.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall, VideoGAN and VectorGAN provide tangible results in generating videos of human pose. However, the frames generated VideoGAN were of very low resolution as computing images of stick figures is still very difficult. VectorGAN had much better results as we could animate together distinct key points to see the figure move more clearly. Our conclusion is that generating human pose is possible with VectorGAN, but we don’t have nearly enough training data. The original VideoGAN paper used over 35 million clips while we used only 84 videos for training our models. However, the results we got are promising. We only used the ballet dances from the dataset, but it would be interesting (and very time consuming) to see how the model does on other dances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring and Understanding Fourier Transforms using p5.js</title>
      <link>https://amritamar.github.io/project/fouriertransforms/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/fouriertransforms/</guid>
      <description>&lt;p&gt;This was a project I made after starting to learn javascript using &lt;a href=&#34;https://p5js.org/&#34; target=&#34;_blank&#34;&gt;p5.js&lt;/a&gt;. I really like the idea of how code can be used to visualize abstract math concepts (and show what exactly is happening in the background) and I hope to continue making more visualizations using code in the future. First a few references: I got interested in fourier transforms after watching &lt;a href=&#34;https://www.youtube.com/watch?v=spUNpyF58BY&#34; target=&#34;_blank&#34;&gt;3Blue1Brown&amp;rsquo;s video on the topic&lt;/a&gt;. I also saw this really &lt;a href=&#34;http://www.jezzamon.com/fourier/index.html&#34; target=&#34;_blank&#34;&gt;cool and interactive introduction&lt;/a&gt; to the topic by &lt;a href=&#34;https://twitter.com/jezzamonn&#34; target=&#34;_blank&#34;&gt;@jezzamonn&lt;/a&gt;. Finally, I learnt javascript (and most of this project) by watching &lt;a href=&#34;https://www.youtube.com/channel/UCvjgXvBlbQiydffZU7m1_aw&#34; target=&#34;_blank&#34;&gt;Daniel Shiffman&amp;rsquo;s Coding Challenges&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;What is a fourier transform? It&amp;rsquo;s simply a way of breaking apart a wave into individual components. This is most used in signal processing, specifically in the audio area. One of the coolest things you can do with fourier transforms is extract specific sounds and tune them however you like! So for example, if you had a music clip with a bit of fuzzy noise in the background, you can use a fourier transform on the clip and get the individual sound waves. After finding the one causing the fuzzy noise, you can remove it. Combine the remaining waves and voila - you have a sound clip without that fuzzy sound.&lt;/p&gt;

&lt;p&gt;Following Daniel Shiffman&amp;rsquo;s tutorial, I first started with visualizing the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_series&#34; target=&#34;_blank&#34;&gt;fourier series&lt;/a&gt;. I visualized the square wave and the sawtooth wave. You can adjust the sliders to increase/reduce the number of epicycles, and use the dropdown menu to change the wave in question.&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/SnFt_PCfq&#34; width=&#34;604&#34; height=&#34;430&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Signals, or waves, can be interpreted as a set of points. One for the X axis, and one for the Y axis. If we apply the fourier transform to both of them, we can 2 transforms that give us the frequence, phase, and amplitude of each component. This is what I did in the next iteration!&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/agLRZQ7Wm&#34; width=&#34;704&#34; height=&#34;404&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I then, instead of simply drawing a circle from a set of points, implemented the ability to create the signal/drawing yourself! Once you finish drawing, the program does a fourier transform, breaking your drawing into X and Y components. Try it out - click and draw something!&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/wCIhlz7Yv&#34; width=&#34;704&#34; height=&#34;404&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The final part of this project involved combining the 2 parts, X and Y components, into 1 Complex Number component. This allows you to use only one overall circle to draw both parts of the transform as the complex number component captures both X and Y of the drawing. Try the final version out!&lt;/p&gt;

&lt;p&gt;In addition:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;use A/D to reduce/increase the number of epicycles used&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;use W/S to increase/reduce the speed&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip; and see how it affects the fourier transform!&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/YjvB9MTas&#34; width=&#34;704&#34; height=&#34;404&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I love the way code can be used to visualize math concepts. I hope to do more of these in the future, both to understand the math behind tricky concepts and to improve my javascript skills!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ultimate Tic Tac Toe!</title>
      <link>https://amritamar.github.io/project/ultimatetictactoe/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/ultimatetictactoe/</guid>
      <description>&lt;p&gt;This is an old project I recently ported over to JS. The original code for the project was written in Visual Basic 6, the first language I learnt. I returned to this project while learning JS (specifically using &lt;a href=&#34;https://p5js.org/&#34; target=&#34;_blank&#34;&gt;p5.js&lt;/a&gt;). It&amp;rsquo;s a 2 player game that adds an extra layer of thinking to the original game of tic tac toe.&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/x2dtFTAsH&#34; width=&#34;604&#34; height=&#34;804&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I hope to continue this by adding AI players!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stelliform</title>
      <link>https://amritamar.github.io/project/stelliform/</link>
      <pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/stelliform/</guid>
      <description>&lt;p&gt;&lt;em&gt;A star has fallen from the sky and the race is on to go claim it. However, the monsters around the area will not make it easy. Players can choose to play as a warrior, rogue, or wizard to blast through enemies by choosing powerful spells or attacks and casting them as quickly as possible while dodging enemy blows to stay alive.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Stelliform is a real-time action card game that relies on quick swiping mechanics, split-second decision making, and pre-game planning. The integration of a real-time strategy card game with a stimulating swipe mechanic makes Stelliform a distinct and unique mobile game. By combining strategic card-based elements and real-time action, we want players to step into the shoes of a spellcaster, defeating enemies by constructing a spell deck and planning and casting spells under time pressure.&lt;/p&gt;

&lt;p&gt;The game was produced by Team Sicko Code, consisting of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amrit Amar &lt;em&gt;(Project Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Alan Pascual &lt;em&gt;(Architecture Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Luke Shin &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Brandon Zhang &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Hanna Arfine &lt;em&gt;(UI/UX Artist)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ally Yuan &lt;em&gt;(Character Artist)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Michael Yee &lt;em&gt;(Music)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The game is made using CUGL (Cornell University Graphics Library), a game engine designed by &lt;a href=&#34;https://www.cs.cornell.edu/~wmwhite/&#34; target=&#34;_blank&#34;&gt;Professor Walker White&lt;/a&gt; in C++. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in C++, along with Android Building and testing.&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/a.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/a_hu30ce87c738cffa646e22b2f11a6a3d3e_1640858_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/b.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/b_hu2c7ebc69544b1ab6effe9d3d0b20ae77_1859154_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/c.jpg&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/c_hub968039be8bef3bf8021b0affcd3609b_426512_0x190_resize_q90_lanczos.jpg&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/d.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/d_hubef768a9f12543cbd1fdec8193f4c174_1090578_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/e.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/e_hu80fa136eb35823e713e76336e643c09e_959569_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/f.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/f_hu05e4240577647107b63b243357e8ceb3_987068_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/stelliform/gallery/g.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/stelliform/gallery/g_hu955c44cb1dfe6137d3bdf42c2f411bbb_1374568_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Viridian</title>
      <link>https://amritamar.github.io/project/viridian/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/viridian/</guid>
      <description>&lt;p&gt;&lt;em&gt;You&amp;rsquo;ve crashed on a mysterious world after getting caught in an explosion. You&amp;rsquo;ve lost the shards that allowed you to fly and explore the galaxy. Make your way through different environments to find the shards and escape from the planet!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Viridian is an action webgame made that relies on precise mechanics to make your way through a magical forest. Use your mouse to shoot and gain momentun as you travel through 3 biomes. Beware of shooting plants, however, as the forest is connected to each other and plants will actively try to defend their fellow flora.&lt;/p&gt;

&lt;p&gt;The game was produced by Team LVI Studios, consisting of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amrit Amar &lt;em&gt;(Project Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Alan Pascual &lt;em&gt;(Architecture Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Justin Lue &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Brandon Zhang &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Yanying (Mary) Ji &lt;em&gt;(UI/UX Artist)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Urael Xu &lt;em&gt;(Character Artist)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The game is made using Unity Game Engine. Throughout development, the team released several builds of the game and gathered data using A/B testing on statistics such as player deaths, time spent on levels, and other indicators that the team used to incrementally build a better game. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented the levels and graphics in C#.&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/viridian/gallery/a.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/viridian/gallery/a_hu463d0b80dc2b1879ece1e58bc591e2fb_1272007_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/viridian/gallery/b.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/viridian/gallery/b_hua97c29243ca57062aad9a3eee46f9fae_1268695_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/viridian/gallery/c.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/viridian/gallery/c_hu91fa153f56f3815e358d09132f3ea5b4_604682_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/viridian/gallery/d.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/viridian/gallery/d_hu0812b250b1d9974da690e1aed429203c_188992_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/viridian/gallery/e.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/viridian/gallery/e_hu67edee6745ae5f14af4ebdee30affe49_904560_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genetic Self-Driving Cars</title>
      <link>https://amritamar.github.io/project/geneticselfdrivingcars/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/geneticselfdrivingcars/</guid>
      <description>&lt;p&gt;This is a project I did to learn about Neural Networks and Genetic Algorithms. We create a neural network to drive a car and then use genetic algorithms to train and find the best weights for the neural network. The network takes in 6 inputs and outputs the acceleration and steering of the car.&lt;/p&gt;

&lt;p&gt;Here, we see all the cars beginning to get initialized with a random neural network. They are pretty dumb.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticSelfDrivingCars/master/Gifs/FirstGen.gif&#34; alt=&#34;FG&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the editor, this is the information that cars get (as shown by the yellow lines).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticSelfDrivingCars/master/Gifs/NNView.gif&#34; alt=&#34;NN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By generation 22, we have a car that can finish the whole track (albeit not smoothly)!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticSelfDrivingCars/master/Gifs/BestCar.gif&#34; alt=&#34;BC&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On average, it takes about 22 generations to train the neural network fully.&lt;/p&gt;

&lt;p&gt;The neural network has 6 input nodes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;current speed&lt;/li&gt;
&lt;li&gt;distance of closest obstacle from the left side (max 10m)&lt;/li&gt;
&lt;li&gt;distance of closest obstacle from the right side (max 10m)&lt;/li&gt;
&lt;li&gt;distance of closest obstacle from the front (max 10m)&lt;/li&gt;
&lt;li&gt;distance of closest obstacle from the front-left side (max 10m)&lt;/li&gt;
&lt;li&gt;distance of closest obstacle from the front-right side (max 10m)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is 1 hidden layer, consisting of 5 nodes. There is 1 output layer, consisting of 2 nodes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Acceleration (-1 to 1)&lt;/li&gt;
&lt;li&gt;Steering (-1 to 1)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I use tanH as my activation function.&lt;/p&gt;

&lt;p&gt;There are 3 main scripts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CarController: this is the script for a car. Has a neural network that drives the car.&lt;/li&gt;
&lt;li&gt;CarManager: manager for the population of cars&lt;/li&gt;
&lt;li&gt;NeuralNetwork: holds the neural network code&lt;/li&gt;
&lt;li&gt;GeneticAlgorithmController: performs the selection process of the algorithm
The program works as follows:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The input and hidden layer nodes have both a weight and a bias, that is randomly generated for the population of cars (default 10). They are then simulated as generation 1 on the race track. After all the cars crash or 60 seconds, the algorithm chooses the best 2 cars (based on displacement from starting point) and then creates the next generation of cars by combining the best 2 cars genes (randomly selecting between the 2 or a combination thereof). There is an option that allows us to keep the best 2 cars in the next generation or to generate all new cars in the population from the parents (i.e. the parents die). The next generation is then simulated and so on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Particle Simulation of Life</title>
      <link>https://amritamar.github.io/project/particlesimulationoflife/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/particlesimulationoflife/</guid>
      <description>&lt;p&gt;This is a JS implementation of a particle system. The project simulates particles in a &amp;lsquo;box&amp;rsquo; and assigns random forces between different types of particles. Interestingly, certain behaviors arise that are &amp;lsquo;life-like&amp;rsquo; from these rules. The particles interact with each other arising in behaviors that mimic cell behavior.&lt;/p&gt;

&lt;p&gt;This simulation works by having each particle type have a certain force associated with another particle type. Sometimes the force can attract, sometimes it repels. The force is stronger the closer the two particles are, but after a certain point, they do not affect each other. The randomization of these forces leads to a number of interesting behaviors. You can play with the simulation in the widget below!&lt;/p&gt;

&lt;p&gt;You can control the number of particles (and click to add more of a certain type), the interaction radius controls the distance at which particles interact with each other (a global variable here), and a few other options. Feel free to edit the code and try out other values in the &lt;a href=&#34;https://editor.p5js.org/AmritAmar/sketches/-L9i8L96T&#34; target=&#34;_blank&#34;&gt;web code editor&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;embed p5js script&#34; &gt;
    &lt;iframe src=&#34;https://editor.p5js.org/AmritAmar/embed/-L9i8L96T&#34; width=&#34;724&#34; height=&#34;804&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I was inspired by this from an Evolution Class I took at Cornell where I read the theories of biologist Lynn Margulis. I was further inspired to create my own implementation from Jeffrey Ventrella&amp;rsquo;s &lt;a href=&#34;http://www.ventrella.com/Clusters/&#34; target=&#34;_blank&#34;&gt;Clusters&lt;/a&gt;, which has examples in both the browser and in augmented reality. Furthermore, I also drew inspiration from &lt;a href=&#34;http://zool33.uni-graz.at/artlife/PPS&#34; target=&#34;_blank&#34;&gt;this study&lt;/a&gt; and &lt;a href=&#34;https://github.com/HackerPoet/&#34; target=&#34;_blank&#34;&gt;HackerPoet&lt;/a&gt; and his &lt;a href=&#34;https://www.youtube.com/watch?v=Z_zmZ23grXE&#34; target=&#34;_blank&#34;&gt;Particle-Life Video&lt;/a&gt;. The main idea, from &lt;a href=&#34;https://evolution.berkeley.edu/evolibrary/article/history_24&#34; target=&#34;_blank&#34;&gt;Lynn Margulis&lt;/a&gt; is that all organisms naturally gravitate to other kinds of organisms, and likewise are repelled by different organisms. These particles experience attractions and repulsions with other particles of different colors. They cluster into social pods, or scatter and flee, often mimicking biological behaviors. Clusters shapeshift and exchange parts of their identities, much like the tiny organisms that came together in early evolution to form symbiotic unions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OutOfSync</title>
      <link>https://amritamar.github.io/project/outofsync/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/outofsync/</guid>
      <description>&lt;p&gt;&lt;em&gt;You are a lab assistant who has accidentally shattered the fabric of reality and created numerous rifts between worlds. Time, gravity, and even color have been drained from the world. Now you must travel through these rifts, recover the scattered shards of
space, and restore them to the riftss in order to fix the damage to reality.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;OutOfSync is a action, strategy platformer that relies on quick decision making and mechanics. You must plan your route and be quick as you collect the shards, return back to repair the rifts, and move to the next level. However, as you are doing this, clones of you will take your exact path - collide with them and you lose the game.&lt;/p&gt;

&lt;p&gt;The game was produced by Team Tektite, consisting of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amrit Amar &lt;em&gt;(Project Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Alan Pascual &lt;em&gt;(Architecture Lead)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Luke Shin &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Brandon Zhang &lt;em&gt;(Programmer)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Cathy Liu &lt;em&gt;(UI/UX Artist)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Sarah Skrutskie &lt;em&gt;(Character Artist)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to Adam Pascual for an amazing soundtrack!&lt;/p&gt;

&lt;p&gt;The game is made using LibGDX and Box2D. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in Java, along with building and testing the game.&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/outofsync/gallery/a.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/outofsync/gallery/a_hude9a43581b792d18a7b7a457d5421db0_85400_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/outofsync/gallery/b.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/outofsync/gallery/b_huc0fadea28317b51dfd0645f4c32f7176_315784_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/outofsync/gallery/c.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/outofsync/gallery/c_hu72dabc5fa29dc949485dfde7b1159ea4_201207_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/outofsync/gallery/d.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/outofsync/gallery/d_hue157b7315240659e0bf29e78f04e32ea_323754_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://amritamar.github.io/project/outofsync/gallery/e.png&#34; &gt;
  &lt;img src=&#34;https://amritamar.github.io/project/outofsync/gallery/e_hucbaae8edaeb8b2f5ce062b9ff763bf8e_383992_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genetic Images</title>
      <link>https://amritamar.github.io/project/geneticimages/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/geneticimages/</guid>
      <description>

&lt;p&gt;This is a project I made for my Artificial Intelligence Class under the Team &amp;ldquo;Real Intelligence&amp;rdquo;. It is a image generation program that uses primitive shapes to generate images using genetic algorithms.&lt;/p&gt;

&lt;p&gt;Our intention was to complete a project exploring the concept of genetic algorithms within artificial intelligence. Genetic Algorithms use the concept of natural selection to find a minima to a function. In this case, the genetic algorithm tries to replicate a picture using only primitive shapes and tries to bring the generated picture as close to the real picture as possible.&lt;/p&gt;

&lt;p&gt;To generate the image, we can adjust the number of shapes, the mutation rate, and the types/combinations of shapes to use in the generated image:
- Circles (mutate the position, radius, color)
- Ellipses (mutate the position, width, height, color)
- Rectangles (mutate the position, width, height, color)
- Pixels (n-pixels long squares, mutate the pixel position, color)
- n-sided polygons&lt;/p&gt;

&lt;p&gt;Our implementation focuses on using a single-parent. This means that during each generation, only one picture is used to create all the children. A parent is replaced when one of the children has a better fitness score (or worse, rather). Our fitness function work by comparing the child to the original image pixel by pixel.&lt;/p&gt;

&lt;p&gt;The pseudocode for our genetic algorithm is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Generate a random image based on user-defined configurations, mainly number of shapes and types of shapes to use. Every ‘individual’ image has a ‘DNA’ that is a collection of shapes. These shapes are randomly generated and are assigned random colors. This individual image’s fitness score is also calculated here.&lt;/li&gt;
&lt;li&gt;We then generate a new image with a certain mutation rate. This new image will have random differences from the parent image (differences ranging from changing the shapes’ position or dimensions to changing the shapes’ color).&lt;/li&gt;
&lt;li&gt;The new image is then compared with the original image. We then compare the new image’s fitness score with the parent’s fitness score. If the image’s fitness score is lower than that of the parent image (i.e, there is less error in this image, thus the new image is closer to the original picture than the parent image), then we update the parent image to be the new image. If the fitness score is higher than the parent’s score, then we simply keep the same parent for the next generation.&lt;/li&gt;
&lt;li&gt;Go to part 2 and repeat until fitness score of the parent is 0.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;example-1-eiffel-tower&#34;&gt;Example 1: Eiffel Tower:&lt;/h3&gt;

&lt;h4 id=&#34;target-picture&#34;&gt;Target Picture:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/eiffel2.png&#34; alt=&#34;EFOG&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;starting-the-algorithm&#34;&gt;Starting the Algorithm:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/EiffelStart.gif&#34; alt=&#34;EF1&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;after-a-bit-of-time&#34;&gt;After a bit of time:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/EiffelMedium.gif&#34; alt=&#34;EF2&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;after-a-long-time&#34;&gt;After a long time:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/EiffelEnd.gif&#34; alt=&#34;EF3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the sky gradient is captured by the generated picture, simply by the adjustment of the alpha of overlapping shapes!&lt;/p&gt;

&lt;p&gt;Over time, the error in the generated images decreases. Of course, this happens over thousands of generations. As we used our program with a multitude of images, we noticed that some shapes converge to target images better than other shapes. For the full evaluation, check out our &lt;a href=&#34;https://drive.google.com/file/d/18TFZThbFswRllVY08Np8gfi7XHtzdqvD/view&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example-2-mona-lisa&#34;&gt;Example 2: Mona Lisa:&lt;/h3&gt;

&lt;h4 id=&#34;target-picture-1&#34;&gt;Target Picture:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/mona_lisa.jpg&#34; alt=&#34;EFOG&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;starting-the-algorithm-1&#34;&gt;Starting the Algorithm:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/MonaBegin.gif&#34; alt=&#34;EF1&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;after-a-bit-of-time-1&#34;&gt;After a bit of time:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/MonaMedium.gif&#34; alt=&#34;EF2&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;after-a-long-time-1&#34;&gt;After a long time:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AmritAmar/GeneticImages/master/Gifs/MonaEnd.gif&#34; alt=&#34;EF3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly, the algorithm seems to capture the background really well! Of course, facial features will take quite a while to develop as there is not much of a pixel difference from this image and one with facial features.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Hand</title>
      <link>https://amritamar.github.io/project/the-hand/</link>
      <pubDate>Sun, 21 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/project/the-hand/</guid>
      <description>&lt;p&gt;This is a hand that can be used to do a lot of things. The basic motive of the project is that the user will wear the control glove, and will be able to control the robotic hand using their movements. A close friend and I made a robotic animatronic hand that could potentially help save lives in the event of an accident in Mines, one of Botswana&amp;rsquo;s main industries.&lt;/p&gt;

&lt;p&gt;There are many uses for this such as Space Exploration and in the Mining Industry. Instead of using AI, that can&amp;rsquo;t react to all situations in an environment, we can use robots, that are controlled by the user. Humans can react to sudden environmental changes and, using &amp;ldquo;The Hand&amp;rdquo;, they can improvise and adapt.&lt;/p&gt;

&lt;p&gt;For making the Project, we used an Arduino Leonardo, Flex Sensors and Servo Motors. The Arduino is a microprocessor that basically reads from code uploaded to it from a computer (C/C++ Code) and therefore through the use of Digital and Analog I/O pins, executes the code. Flex Sensors are Variable Resistors that change values depending on how much they are bent by. Servo Motors are precision DC motors that can be controlled to point to the exact angle from 0 to 180 Degrees.&lt;/p&gt;

&lt;p&gt;What basically happens here is that the Flex Sensor readings are inputs through the Analog Pins. It is processed and mapped to a angle. The angle is then written to the Servo Motors, thus turning them and therefore moving the fingers. There are also two buttons that control a LED and a Speaker. On the breadboard, there is an LDR (Light dependent resistor) that basically measure the amount of light in a room. Using those values, it gives a green light or a yellow light. The Arduino can be used to do a lot of things!&lt;/p&gt;

&lt;p&gt;In the future, we hope to develop a better working hand with all fingers working and an opposable thumb. This would allow us to hold things. The next stage of this project is to finish the hand and make sure it can pick up things.&lt;/p&gt;

&lt;p&gt;We won first place at a National Competition (The Botho College ICT Linkz Challenge 2014) with this project. Thank you to everyone who helped out!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TA5GpAQBzUY&#34; target=&#34;_blank&#34;&gt;A Video Demonstration can be found here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://amritamar.github.io/moralsupport/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/moralsupport/</guid>
      <description>&lt;div style=&#34;text-align: center; margin-top: 50px;&#34;&gt;
            &lt;h1 style=&#34;font-family: arial; font-size: 50px; text-shadow: 3px 3px 10px #aaa&#34;&gt;
                &lt;span style=&#34;color:#ff0000;&#34;&gt;Y&lt;/span&gt;&lt;span style=&#34;color:#ff4000;&#34;&gt;o&lt;/span&gt;&lt;span style=&#34;color:#ff7f00;&#34;&gt;u&lt;/span&gt;&lt;span style=&#34;color:#ffbf00;&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ffff00;&#34;&gt;c&lt;/span&gt;&lt;span style=&#34;color:#aaff00;&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#55ff00;&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#00ff00;&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#00ff80;&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#00ffff;&#34;&gt;o&lt;/span&gt;&lt;span style=&#34;color:#0080ff;&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#0000ff;&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#4600ff;&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#8b00ff;&#34;&gt;!&lt;/span&gt;
            &lt;/h1&gt;
            &lt;img class=&#34;noselect&#34; draggable=&#34;false&#34; src=&#34;https://i.imgur.com/anqcRxv.gif&#34; /&gt;
        &lt;/div&gt;


&lt;p&gt;I believe in you! If you can&amp;rsquo;t believe in yourself, believe in the me who believes in you!  You&amp;rsquo;re handsome and you&amp;rsquo;re smart and you&amp;rsquo;re the best.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://amritamar.github.io/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AmritAmar</title>
      <link>https://amritamar.github.io/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://amritamar.github.io/experience/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
